{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3BCBLWEHPxeW"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anddennn/IAT360_YogaPoseDetection_CVProject/blob/main/ComputerVisionProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> <b> YOLO-NAS-POSE Yoga Detection Model <b> </h1></center>"
      ],
      "metadata": {
        "id": "URt4Q-TNmNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this youtube video as reference: https://www.youtube.com/watch?v=J83ZvWfxjoA\n",
        "\n",
        "Code snippets from video's linked google colab folder."
      ],
      "metadata": {
        "id": "f4zJQE__KpaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required Libraries"
      ],
      "metadata": {
        "id": "rr-j-jCmaHId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!apt-get install -y build-essential\n",
        "!apt-get -y install libfluidsynth-dev\n",
        "\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "\n",
        "!pip install super-gradients\n",
        "!pip install -qq gdown torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzmALoxKXZh",
        "outputId": "224a2ee5-fb89-4aea-9200-937b1e0ce3ac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  gir1.2-ibus-1.0 libdbus-1-dev libdecor-0-dev libdrm-dev libegl-dev\n",
            "  libegl1-mesa-dev libfluidsynth3 libgbm-dev libgl-dev libgles-dev libgles1\n",
            "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n",
            "  libibus-1.0-5 libibus-1.0-dev libinstpatch-1.0-2 libinstpatch-dev\n",
            "  libopengl-dev libpciaccess-dev libpulse-dev libpulse-mainloop-glib0\n",
            "  libsdl2-dev libsndio-dev libsystemd-dev libudev-dev libudev1 libwayland-bin\n",
            "  libwayland-dev libxcursor-dev libxfixes-dev libxi-dev libxinerama-dev\n",
            "  libxkbcommon-dev libxrandr-dev libxt-dev libxv-dev libxxf86vm-dev\n",
            "  timgm6mb-soundfont\n",
            "Suggested packages:\n",
            "  libwayland-doc libxt-doc fluid-soundfont-gm\n",
            "The following NEW packages will be installed:\n",
            "  gir1.2-ibus-1.0 libdbus-1-dev libdecor-0-dev libdrm-dev libegl-dev\n",
            "  libegl1-mesa-dev libfluidsynth-dev libfluidsynth3 libgbm-dev libgl-dev\n",
            "  libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libibus-1.0-5 libibus-1.0-dev libinstpatch-1.0-2\n",
            "  libinstpatch-dev libopengl-dev libpciaccess-dev libpulse-dev\n",
            "  libpulse-mainloop-glib0 libsdl2-dev libsndio-dev libsystemd-dev libudev-dev\n",
            "  libwayland-bin libwayland-dev libxcursor-dev libxfixes-dev libxi-dev\n",
            "  libxinerama-dev libxkbcommon-dev libxrandr-dev libxt-dev libxv-dev\n",
            "  libxxf86vm-dev timgm6mb-soundfont\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 41 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 10.7 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.17 [76.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibus-1.0-5 amd64 1.5.26-4 [183 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 gir1.2-ibus-1.0 amd64 1.5.26-4 [88.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdbus-1-dev amd64 1.12.20-2ubuntu4.1 [188 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-bin amd64 1.20.0-1ubuntu0.1 [20.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-dev amd64 1.20.0-1ubuntu0.1 [69.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-dev amd64 0.1.0-3build1 [5,544 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpciaccess-dev amd64 0.16-3 [21.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-dev amd64 2.4.113-2~ubuntu0.22.04.1 [292 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libegl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [11.1 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-dev amd64 1.1.6-1 [54.4 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse-mainloop-glib0 amd64 1:15.99.1+dfsg1-1ubuntu2.2 [12.4 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse-dev amd64 1:15.99.1+dfsg1-1ubuntu2.2 [75.6 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgbm-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [9,542 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibus-1.0-dev amd64 1.5.26-4 [185 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsndio-dev amd64 1.8.1-1.1 [17.8 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev-dev amd64 249.11-0ubuntu3.17 [20.7 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes-dev amd64 1:6.0.0-1 [12.2 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcursor-dev amd64 1:1.2.0-2build4 [28.2 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi-dev amd64 2:1.8-1build1 [193 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxinerama-dev amd64 2:1.1.4-3 [8,104 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-dev amd64 1.4.0-1 [54.9 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrandr-dev amd64 2:1.5.2-1build1 [26.7 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxv-dev amd64 2:1.0.11-1build2 [33.4 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86vm-dev amd64 1:1.1.4-1build3 [13.9 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsdl2-dev amd64 2.0.20+dfsg-2ubuntu1.22.04.1 [1,767 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsystemd-dev amd64 249.11-0ubuntu3.17 [306 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth-dev amd64 2.2.5-1 [35.1 kB]\n",
            "Fetched 10.7 MB in 2s (4,555 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "(Reading database ... 126718 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\n",
            "(Reading database ... 126718 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libibus-1.0-5_1.5.26-4_amd64.deb ...\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\n",
            "Preparing to unpack .../01-gir1.2-ibus-1.0_1.5.26-4_amd64.deb ...\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\n",
            "Preparing to unpack .../02-libdbus-1-dev_1.12.20-2ubuntu4.1_amd64.deb ...\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Selecting previously unselected package libwayland-bin.\n",
            "Preparing to unpack .../03-libwayland-bin_1.20.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libwayland-bin (1.20.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libwayland-dev:amd64.\n",
            "Preparing to unpack .../04-libwayland-dev_1.20.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libwayland-dev:amd64 (1.20.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libdecor-0-dev:amd64.\n",
            "Preparing to unpack .../05-libdecor-0-dev_0.1.0-3build1_amd64.deb ...\n",
            "Unpacking libdecor-0-dev:amd64 (0.1.0-3build1) ...\n",
            "Selecting previously unselected package libpciaccess-dev:amd64.\n",
            "Preparing to unpack .../06-libpciaccess-dev_0.16-3_amd64.deb ...\n",
            "Unpacking libpciaccess-dev:amd64 (0.16-3) ...\n",
            "Selecting previously unselected package libdrm-dev:amd64.\n",
            "Preparing to unpack .../07-libdrm-dev_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libdrm-dev:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../08-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../09-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../10-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../11-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../12-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../13-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../14-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../15-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../16-libegl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../17-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../18-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../19-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package libinstpatch-dev:amd64.\n",
            "Preparing to unpack .../20-libinstpatch-dev_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-dev:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "Preparing to unpack .../21-libpulse-mainloop-glib0_1%3a15.99.1+dfsg1-1ubuntu2.2_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../22-libpulse-dev_1%3a15.99.1+dfsg1-1ubuntu2.2_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Selecting previously unselected package libgbm-dev:amd64.\n",
            "Preparing to unpack .../23-libgbm-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgbm-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../24-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../25-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\n",
            "Preparing to unpack .../26-libibus-1.0-dev_1.5.26-4_amd64.deb ...\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package libsndio-dev:amd64.\n",
            "Preparing to unpack .../27-libsndio-dev_1.8.1-1.1_amd64.deb ...\n",
            "Unpacking libsndio-dev:amd64 (1.8.1-1.1) ...\n",
            "Selecting previously unselected package libudev-dev:amd64.\n",
            "Preparing to unpack .../28-libudev-dev_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libudev-dev:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package libxfixes-dev:amd64.\n",
            "Preparing to unpack .../29-libxfixes-dev_1%3a6.0.0-1_amd64.deb ...\n",
            "Unpacking libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../30-libxcursor-dev_1%3a1.2.0-2build4_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.2.0-2build4) ...\n",
            "Selecting previously unselected package libxi-dev:amd64.\n",
            "Preparing to unpack .../31-libxi-dev_2%3a1.8-1build1_amd64.deb ...\n",
            "Unpacking libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../32-libxinerama-dev_2%3a1.1.4-3_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.4-3) ...\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\n",
            "Preparing to unpack .../33-libxkbcommon-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../34-libxrandr-dev_2%3a1.5.2-1build1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.2-1build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../35-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package libxv-dev:amd64.\n",
            "Preparing to unpack .../36-libxv-dev_2%3a1.0.11-1build2_amd64.deb ...\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1build2) ...\n",
            "Selecting previously unselected package libxxf86vm-dev:amd64.\n",
            "Preparing to unpack .../37-libxxf86vm-dev_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libxxf86vm-dev:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\n",
            "Preparing to unpack .../38-libsdl2-dev_2.0.20+dfsg-2ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libsdl2-dev:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
            "Selecting previously unselected package libsystemd-dev:amd64.\n",
            "Preparing to unpack .../39-libsystemd-dev_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libsystemd-dev:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package libfluidsynth-dev:amd64.\n",
            "Preparing to unpack .../40-libfluidsynth-dev_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth-dev:amd64 (2.2.5-1) ...\n",
            "Setting up libsndio-dev:amd64 (1.8.1-1.1) ...\n",
            "Setting up libpciaccess-dev:amd64 (0.16-3) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxxf86vm-dev:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxkbcommon-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libibus-1.0-5:amd64 (1.5.26-4) ...\n",
            "Setting up libgbm-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1build2) ...\n",
            "Setting up libwayland-bin (1.20.0-1ubuntu0.1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.2-1build1) ...\n",
            "Setting up libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libudev-dev:amd64 (249.11-0ubuntu3.17) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.4-3) ...\n",
            "Setting up libpulse-dev:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.26-4) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libsystemd-dev:amd64 (249.11-0ubuntu3.17) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libdrm-dev:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.2.0-2build4) ...\n",
            "Setting up libwayland-dev:amd64 (1.20.0-1ubuntu0.1) ...\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.26-4) ...\n",
            "Setting up libdecor-0-dev:amd64 (0.1.0-3build1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libinstpatch-dev:amd64 (1.1.6-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libsdl2-dev:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
            "Setting up libfluidsynth-dev:amd64 (2.2.5-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n",
            "E: Package 'libfluidsynth1' has no installation candidate\n",
            "Collecting super-gradients\n",
            "  Using cached super_gradients-3.7.1-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (4.67.1)\n",
            "Collecting boto3>=1.17.15 (from super-gradients)\n",
            "  Using cached boto3-1.40.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (4.25.1)\n",
            "Collecting Deprecated>=1.2.11 (from super-gradients)\n",
            "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (1.16.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (3.10.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (5.9.5)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.19.0)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (80.9.0)\n",
            "Collecting coverage~=5.3.1 (from super-gradients)\n",
            "  Using cached coverage-5.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (0.23.0+cu126)\n",
            "Collecting sphinx~=4.0.2 (from super-gradients)\n",
            "  Using cached Sphinx-4.0.3-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting sphinx-rtd-theme (from super-gradients)\n",
            "  Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting torchmetrics==0.8 (from super-gradients)\n",
            "  Using cached torchmetrics-0.8.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting hydra-core>=1.2.0 (from super-gradients)\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of super-gradients to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting super-gradients\n",
            "  Using cached super_gradients-3.7.0-py3-none-any.whl.metadata (41 kB)\n",
            "  Using cached super_gradients-3.6.1-py3-none-any.whl.metadata (41 kB)\n",
            "  Using cached super_gradients-3.6.0-py3-none-any.whl.metadata (40 kB)\n",
            "  Using cached super_gradients-3.5.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (4.12.0.88)\n",
            "  Using cached super_gradients-3.4.1-py3-none-any.whl.metadata (37 kB)\n",
            "  Using cached super_gradients-3.4.0-py3-none-any.whl.metadata (37 kB)\n",
            "  Using cached super_gradients-3.3.1-py3-none-any.whl.metadata (37 kB)\n",
            "INFO: pip is still looking at multiple versions of super-gradients to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached super_gradients-3.3.0-py3-none-any.whl.metadata (37 kB)\n",
            "  Using cached super_gradients-3.2.1-py3-none-any.whl.metadata (35 kB)\n",
            "  Using cached super_gradients-3.2.0-py3-none-any.whl.metadata (35 kB)\n",
            "  Using cached super_gradients-3.1.3-py3-none-any.whl.metadata (35 kB)\n",
            "  Using cached super_gradients-3.1.2-py3-none-any.whl.metadata (35 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached super_gradients-3.1.1-py3-none-any.whl.metadata (36 kB)\n",
            "  Using cached super_gradients-3.1.0-py3-none-any.whl.metadata (36 kB)\n",
            "  Using cached super_gradients-3.0.9-py3-none-any.whl.metadata (34 kB)\n",
            "  Using cached super_gradients-3.0.8-py3-none-any.whl.metadata (32 kB)\n",
            "  Using cached super_gradients-3.0.7-py3-none-any.whl.metadata (32 kB)\n",
            "  Using cached super_gradients-3.0.6-py3-none-any.whl.metadata (30 kB)\n",
            "  Using cached super_gradients-3.0.5-py3-none-any.whl.metadata (30 kB)\n",
            "  Using cached super_gradients-3.0.4-py3-none-any.whl.metadata (30 kB)\n",
            "  Using cached super_gradients-3.0.3-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pillow>=9.2.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (11.3.0)\n",
            "Collecting onnxruntime (from super-gradients)\n",
            "  Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting onnx>=1.10.1 (from super-gradients)\n",
            "  Using cached onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting pip-tools>=6.4.0 (from super-gradients)\n",
            "  Using cached pip_tools-7.5.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting pyparsing==2.4.5 (from super-gradients)\n",
            "  Using cached pyparsing-2.4.5-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting einops==0.3.2 (from super-gradients)\n",
            "  Using cached einops-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pycocotools==2.0.4 (from super-gradients)\n",
            "  Using cached pycocotools-2.0.4.tar.gz (106 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting protobuf~=3.19.0 (from super-gradients)\n",
            "  Using cached protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)\n",
            "Collecting treelib==1.6.1 (from super-gradients)\n",
            "  Using cached treelib-1.6.1-py3-none-any.whl\n",
            "Collecting termcolor==1.1.0 (from super-gradients)\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (25.0)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (0.45.1)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools==2.0.4->super-gradients) (2.0.2)\n",
            "Collecting pyDeprecate==0.3.* (from torchmetrics==0.8->super-gradients)\n",
            "  Using cached pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from treelib==1.6.1->super-gradients) (1.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (3.1.6)\n",
            "Collecting docutils<0.18,>=0.14 (from sphinx~=4.0.2->super-gradients)\n",
            "  Using cached docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (3.0.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.17.0)\n",
            "Collecting alabaster<0.8,>=0.7 (from sphinx~=4.0.2->super-gradients)\n",
            "  Using cached alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.32.4)\n",
            "Collecting botocore<1.41.0,>=1.40.59 (from boto3>=1.17.15->super-gradients)\n",
            "  Using cached botocore-1.40.59-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.17.15->super-gradients)\n",
            "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.17.15->super-gradients)\n",
            "  Using cached s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.59->boto3>=1.17.15->super-gradients) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.59->boto3>=1.17.15->super-gradients) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.59->boto3>=1.17.15->super-gradients) (1.17.0)\n",
            "Collecting wrapt<2,>=1.10 (from Deprecated>=1.2.11->super-gradients)\n",
            "  Using cached wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.2.0->super-gradients) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf->super-gradients) (6.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.3->sphinx~=4.0.2->super-gradients) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (0.27.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (1.4.9)\n",
            "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting onnx>=1.10.1 (from super-gradients)\n",
            "  Using cached onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "  Using cached onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Using cached onnx-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.15.0.tar.gz (12.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is still looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached onnx-1.14.1.tar.gz (11.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.14.0.tar.gz (11.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.13.1.tar.gz (10.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.13.0.tar.gz (10.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.12.0.tar.gz (10.1 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.10.1->super-gradients) (4.15.0)\n",
            "Requirement already satisfied: build>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (1.3.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (8.3.0)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (25.3)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (2025.10.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9.0->super-gradients) (1.3.0)\n",
            "Collecting coloredlogs (from onnxruntime->super-gradients)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->super-gradients) (25.9.23)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->super-gradients)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "INFO: pip is looking at multiple versions of sphinx-rtd-theme to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinx-rtd-theme (from super-gradients)\n",
            "  Using cached sphinx_rtd_theme-3.0.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "  Using cached sphinx_rtd_theme-3.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "  Using cached sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "  Using cached sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->super-gradients)\n",
            "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Using cached super_gradients-3.0.3-py3-none-any.whl (732 kB)\n",
            "Using cached einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Using cached pyparsing-2.4.5-py2.py3-none-any.whl (67 kB)\n",
            "Using cached torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
            "Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
            "Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Using cached Sphinx-4.0.3-py3-none-any.whl (2.9 MB)\n",
            "Using cached alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
            "Using cached docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "Using cached boto3-1.40.59-py3-none-any.whl (139 kB)\n",
            "Using cached botocore-1.40.59-py3-none-any.whl (14.1 MB)\n",
            "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Using cached s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Using cached wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Using cached pip_tools-7.5.1-py3-none-any.whl (65 kB)\n",
            "Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Using cached sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
            "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "Building wheels for collected packages: pycocotools, onnx\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycocotools \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m No available output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pycocotools\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for onnx \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m No available output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for onnx (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for onnx\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pycocotools onnx\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
            "\u001b[31m╰─>\u001b[0m pycocotools, onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4373aa2",
        "outputId": "04deb60c-65f3-400b-fd74-8926eb379ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycocotools==2.0.4 --global-option=\"build_ext\" --global-option=\"-I/usr/include/python3.10\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip install [options] [-e] <vcs project url> ...\n",
            "  pip install [options] [-e] <local project path> ...\n",
            "  pip install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --global-option\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Tuple, Union\n",
        "\n",
        "# Third-party imports\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from IPython.display import YouTubeVideo\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Colab specific imports\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Constants\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "3_BT3x6HaFLn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training import models\n",
        "from super_gradients.common.object_names import Models\n",
        "\n",
        "yolo_nas_pose = models.get(\"yolo_nas_pose_l\", pretrained_weights=\"coco_pose\").cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "3lxnfMWALE9z",
        "outputId": "94691353-6734-423d-b701-1641e2db7ca8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'super_gradients'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3402013858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_names\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myolo_nas_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo_nas_pose_l\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"coco_pose\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3FGskainy8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "For preprocessing:\n",
        "- First manually sorted through the images to remove invalid data (i.e. clipart, children, non-yoga poses, etc.)\n",
        "- Added more diversity to the dataset\n",
        "- After annotation, used roboflow to normalize all data into 640x640 jpg images that are auto oriented for better performance\n",
        "- Augmented data with roboflow to mimic indoor conditions such as adding more low light data or low resolution data.\n"
      ],
      "metadata": {
        "id": "7aQjNM0hDaBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training import Trainer\n",
        "\n",
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "trainer = Trainer(experiment_name='first_yn_pose_run', ckpt_root_dir=CHECKPOINT_DIR)"
      ],
      "metadata": {
        "id": "HpNoSkncPfQw",
        "outputId": "892eb270-d1b3-4df0-c8a2-7a485ce18bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'super_gradients'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3991803953.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCHECKPOINT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first_yn_pose_run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_root_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset"
      ],
      "metadata": {
        "id": "8baBWY6dPjiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7kHMqGVPPo1Q",
        "outputId": "298dec18-59c6-47fe-80c8-e7a9845bf9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "# fix path to dataset\n",
        "!cp -r \"/content/drive/MyDrive/Dataset\" \"/content/\""
      ],
      "metadata": {
        "id": "YiIr3J6rPpWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"1OkaP72I2Cr9MhYbZuXi\")\n",
        "project = rf.workspace(\"yoga-pose-dataset\").project(\"yoga-poses-yiqyx\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")\n"
      ],
      "metadata": {
        "id": "Uyis1GKeKMgT",
        "outputId": "06ff96cc-76db-4af1-fdd6-c2672f5e6c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "idna"
                ]
              },
              "id": "3503b6566d104be7bc4e12d36af271eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Yoga-Poses-1 to coco:: 100%|██████████| 22700/22700 [00:00<00:00, 40170.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Yoga-Poses-1 in coco:: 100%|██████████| 662/662 [00:00<00:00, 7531.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get yaml file for standard coco keypoints.\n",
        "!wget https://raw.githubusercontent.com/Deci-AI/super-gradients/master/src/super_gradients/recipes/dataset_params/coco_pose_estimation_common_dataset_params.yaml"
      ],
      "metadata": {
        "id": "ljdUO37IQDer",
        "outputId": "0d4966b8-fd03-45b0-ea3e-42d3325d44f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-25 21:53:41--  https://raw.githubusercontent.com/Deci-AI/super-gradients/master/src/super_gradients/recipes/dataset_params/coco_pose_estimation_common_dataset_params.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1937 (1.9K) [text/plain]\n",
            "Saving to: ‘coco_pose_estimation_common_dataset_params.yaml.1’\n",
            "\n",
            "\r          coco_pose   0%[                    ]       0  --.-KB/s               \rcoco_pose_estimatio 100%[===================>]   1.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-25 21:53:42 (48.4 MB/s) - ‘coco_pose_estimation_common_dataset_params.yaml.1’ saved [1937/1937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def open_file(file_path: str) -> Union[dict, list, None]:\n",
        "    \"\"\"\n",
        "    Opens and reads the content of a JSON or YAML file.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): The path to the file.\n",
        "\n",
        "    Returns:\n",
        "    Union[dict, list, None]: The content of the file parsed to a dictionary or a list,\n",
        "                             or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            if file_path.endswith('.json'):\n",
        "                return json.load(file)\n",
        "            elif file_path.endswith('.yaml') or file_path.endswith('.yml'):\n",
        "                return yaml.safe_load(file)\n",
        "            else:\n",
        "                raise ValueError(f'Unsupported file format: {file_path}')\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred: {e}')\n",
        "        return None\n",
        "\n",
        "# Fix paths to dataset ###########\n",
        "\n",
        "annotations = open_file('/content/drive/MyDrive/Dataset/train/_annotations.coco.json')\n",
        "config = open_file('/content/drive/MyDrive/Dataset/valid/_annotations.coco.json')"
      ],
      "metadata": {
        "id": "VENnHqTzQMU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotation File Breakdown:\n",
        "\n",
        "1. images:\n",
        "It's a dictionary where each entry maps an ID to a filename (typically an image filename). Each image in the dataset has a unique identifier, and this is a lookup between the ID and the filename.\n",
        "\n",
        "2. annotations:\n",
        "This is a list containing 6,117 items. Each item is a dictionary with details related to the annotations for a particular image.\n",
        "Each annotation contains an image_id, a list of keypoints, and num_keypoints value.\n",
        "\n",
        "3. categories:\n",
        "A list of categories for the dataset.\n",
        "Each category has a:\n",
        "- supercategory: A broader classification (like 'animal').\n",
        "- id: A unique identifier for the category.\n",
        "- name: The name of the category (e.g., 'dog', 'cat', 'sheep').\n",
        "- keypoints: A list of names for specific keypoints relevant to that category (like 'left_eye', 'right_eye', 'nose', etc.).\n",
        "- skeleton: A list of pairs, which are connections between keypoints."
      ],
      "metadata": {
        "id": "Y1Xj5N68Qs3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotting a sample of images\n",
        "\n",
        "def plot_random_images(data, image_base_dir=\"/content/images\"):\n",
        "    \"\"\"\n",
        "    Plots 5 random images for each category from the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The JSON dataset containing image, annotation, and category details.\n",
        "    - image_base_dir: The base directory where the images are located.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a dictionary to map image IDs to filenames\n",
        "    image_id_to_filename = {image['id']: image['filename'] for image in data['images']}\n",
        "\n",
        "    # Extracting image_ids for each category\n",
        "    category_image_ids = {}\n",
        "    for category in data['categories']:\n",
        "        category_id = category['id']\n",
        "        category_name = category['name']\n",
        "        category_image_ids[category_name] = [anno['image_id'] for anno in data['annotations'] if anno['category_id'] == category_id]\n",
        "\n",
        "    # Randomly select 5 image_ids for each category\n",
        "    random_selected_ids = {}\n",
        "    for category_name, ids in category_image_ids.items():\n",
        "        random_selected_ids[category_name] = random.sample(ids, min(5, len(ids)))\n",
        "\n",
        "    # Number of categories\n",
        "    num_categories = len(random_selected_ids)\n",
        "\n",
        "    # Create a figure to plot the images\n",
        "    fig, axes = plt.subplots(num_categories, 5, figsize=(20, num_categories * 3))\n",
        "    if num_categories == 1:  # If there is only one category, axes will be 1D\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (category_name, ids) in enumerate(random_selected_ids.items()):\n",
        "        for j, image_id in enumerate(ids):\n",
        "            # Get the filename using the image_id_to_filename dictionary\n",
        "            filename = image_id_to_filename.get(image_id, \"Image_Not_Found.jpg\")\n",
        "\n",
        "            # Load and plot the image\n",
        "            img_path = os.path.join(image_base_dir, filename)\n",
        "            try:\n",
        "                img = mpimg.imread(img_path)\n",
        "                axes[i][j].imshow(img)\n",
        "            except FileNotFoundError:\n",
        "                axes[i][j].imshow(np.zeros((100, 100, 3)))  # Show an empty image if file is not found\n",
        "            axes[i][j].axis('off')\n",
        "            if j == 0:\n",
        "                axes[i][j].set_title(category_name)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3ZQwxy3BQxNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change path for this\n",
        "plot_random_images(data=annotations, image_base_dir=\"/content/drive/MyDrive/Dataset/train\")"
      ],
      "metadata": {
        "id": "Oa9zimu1ROyd",
        "outputId": "049b8e6d-2e9e-4ad9-e744-be44a4f69305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'annotations' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1293616801.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Change path for this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_random_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_base_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Dataset/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'annotations' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets & DataLoaders\n",
        "\n",
        "SuperGradients is fully compatible with PyTorch Datasets and Dataloaders, so you can use your dataloaders as is.\n",
        "\n",
        "### SuperGradients also provides you with the `AbstractPoseEstimationDataset` class.\n",
        "\n",
        "This is an abstract class defines a blueprint for datasets related to pose estimation tasks. It's expected that concrete implementations of this class will be created for specific datasets.\n",
        "\n",
        "- **Inheritance**: It inherits from PyTorch's `Dataset` and `HasPreprocessingParams`.\n",
        "\n",
        "- **Initialization**:\n",
        "  - Takes in parameters like `transforms`, `num_joints`, `edge_links`, `edge_colors`, and `keypoint_colors`.\n",
        "  - Initializes instance variables and constructs a transform pipeline (`KeypointsCompose`).\n",
        "\n",
        "- **Abstract Methods (`__len__` and `load_sample`)**:\n",
        "  - These methods are declared but don't have a concrete implementation in this class. Your derived class from this abstract class is expected to provide an implementation for these methods.\n",
        "  \n",
        "- **`load_random_sample` Method**:\n",
        "  - This method is used to fetch a random sample from the dataset. It uses the `__len__` method to get the total number of samples and then randomly selects an index to retrieve using `load_sample`.\n",
        "\n",
        "- **`__getitem__` Method**:\n",
        "  - This method retrieves a sample given its index. It then applies the defined transformations on the sample and returns it. This method is crucial for PyTorch's DataLoader to fetch samples during training.\n",
        "\n",
        "- **`get_dataset_preprocessing_params` Method**:\n",
        "  - This method defines and returns preprocessing parameters for the dataset. It seems to construct a pipeline of preprocessing steps and their parameters.\n"
      ],
      "metadata": {
        "id": "bkeI_DyoRsu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Expand this cell to see how the `AnimalPoseEstimationDataset` is implemented\n",
        "from super_gradients.common.decorators.factory_decorator import resolve_param\n",
        "from super_gradients.common.factories.target_generator_factory import TargetGeneratorsFactory\n",
        "from super_gradients.common.factories.transforms_factory import TransformsFactory\n",
        "from super_gradients.common.object_names import Datasets\n",
        "from super_gradients.common.registry import register_dataset\n",
        "from super_gradients.training.transforms.keypoint_transforms import AbstractKeypointTransform\n",
        "from super_gradients.training.samples import PoseEstimationSample\n",
        "\n",
        "from super_gradients.training.datasets.pose_estimation_datasets.abstract_pose_estimation_dataset import AbstractPoseEstimationDataset\n",
        "\n",
        "from super_gradients.training.datasets.pose_estimation_datasets import YoloNASPoseCollateFN\n",
        "\n",
        "class PoseEstimationDataset(AbstractPoseEstimationDataset):\n",
        "    \"\"\"\n",
        "    Dataset class for training pose estimation models on Animal Pose dataset.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    @resolve_param(\"transforms\", TransformsFactory())\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        images_dir: str,\n",
        "        json_file: str,\n",
        "        transforms: List[AbstractKeypointTransform],\n",
        "        edge_links: Union[List[Tuple[int, int]], np.ndarray],\n",
        "        edge_colors: Union[List[Tuple[int, int, int]], np.ndarray, None],\n",
        "        keypoint_colors: Union[List[Tuple[int, int, int]], np.ndarray, None],\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        :param data_dir: Root directory of the COCO dataset\n",
        "        :param images_dir: path suffix to the images directory inside the data_dir\n",
        "        :param json_file: path suffix to the json file inside the data_dir\n",
        "        :param include_empty_samples: Not used, but exists for compatibility with COCO dataset config.\n",
        "        :param target_generator: Target generator that will be used to generate the targets for the model.\n",
        "            See DEKRTargetsGenerator for an example.\n",
        "        :param transforms: Transforms to be applied to the image & keypoints\n",
        "        \"\"\"\n",
        "        split_json_file = os.path.join(data_dir, json_file)\n",
        "\n",
        "        with open(split_json_file, \"r\") as f:\n",
        "            json_annotations = json.load(f)\n",
        "\n",
        "\n",
        "        joints = json_annotations[\"categories\"][0][\"keypoints\"]\n",
        "        num_joints = len(joints)\n",
        "\n",
        "        super().__init__(\n",
        "            transforms=transforms,\n",
        "            num_joints=num_joints,\n",
        "            edge_links=edge_links,\n",
        "            edge_colors=edge_colors,\n",
        "            keypoint_colors=keypoint_colors,\n",
        "        )\n",
        "\n",
        "        self.num_joints = num_joints\n",
        "        print(self.num_joints)\n",
        "\n",
        "\n",
        "        images_and_ids = []\n",
        "\n",
        "        for image in json_annotations[\"images\"]:\n",
        "          images_and_ids.append((image[\"id\"], os.path.join(data_dir, images_dir, image[\"filename\"])))\n",
        "        self.image_ids, self.image_files = zip(*images_and_ids)\n",
        "\n",
        "        self.annotations = []\n",
        "\n",
        "        for image_id in self.image_ids:\n",
        "            keypoints_per_image = []\n",
        "            bboxes_per_image = []\n",
        "\n",
        "            image_annotations = [ann for ann in json_annotations[\"annotations\"] if str(ann[\"image_id\"]) == str(image_id)]\n",
        "            for ann in image_annotations:\n",
        "                keypoints = np.array(ann[\"keypoints\"]).reshape(self.num_joints, 3)\n",
        "                x1, y1, x2, y2 = ann[\"bbox\"]\n",
        "\n",
        "                bbox_xywh = np.array([x1, y1, x2 - x1, y2 - y1])\n",
        "                keypoints_per_image.append(keypoints)\n",
        "                bboxes_per_image.append(bbox_xywh)\n",
        "\n",
        "            keypoints_per_image = np.array(keypoints_per_image, dtype=np.float32).reshape(-1, self.num_joints, 3)\n",
        "            bboxes_per_image = np.array(bboxes_per_image, dtype=np.float32).reshape(-1, 4)\n",
        "            annotation = keypoints_per_image, bboxes_per_image\n",
        "            self.annotations.append(annotation)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def load_sample(self, index) -> PoseEstimationSample:\n",
        "        file_path = self.image_files[index]\n",
        "        gt_joints, gt_bboxes = self.annotations[index]  # boxes in xywh format\n",
        "\n",
        "        gt_areas = np.array([box[2] * box[3] for box in gt_bboxes], dtype=np.float32)\n",
        "        gt_iscrowd = np.array([0] * len(gt_joints), dtype=bool)\n",
        "\n",
        "        image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "        mask = np.ones(image.shape[:2], dtype=np.float32)\n",
        "\n",
        "        return PoseEstimationSample(\n",
        "            image=image, mask=mask, joints=gt_joints, areas=gt_areas, bboxes_xywh=gt_bboxes, is_crowd=gt_iscrowd, additional_samples=None\n",
        "        )"
      ],
      "metadata": {
        "id": "tNoeQk-zR5oB",
        "outputId": "c0db7dd2-4872-416b-a853-3561a55f4247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'super_gradients'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-633406369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Expand this cell to see how the `AnimalPoseEstimationDataset` is implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory_decorator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresolve_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_generator_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTargetGeneratorsFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformsFactory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_names\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for annotated files\n",
        "\n",
        "# Change Paths\n",
        "train_annotations = open_file('/content/drive/MyDrive/Dataset/train/')\n",
        "val_annotations = open_file('/content/drive/MyDrive/Dataset/valid/')"
      ],
      "metadata": {
        "id": "uBgKq4CBR-NH",
        "outputId": "bff893d3-1e65-47c6-b386-bd092ef5a71b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'open_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-758084804.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Change Paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Dataset/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_annotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Dataset/valid/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'open_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KeyPoint Tranformations\n",
        "\n",
        "Instantiate the transformations"
      ],
      "metadata": {
        "id": "najn_WPGSHfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Expand this cell to see how the transforms are instantiated\n",
        "from super_gradients.training.transforms.keypoints import (\n",
        "    KeypointsHSV,\n",
        "    KeypointsBrightnessContrast,\n",
        "    KeypointsMosaic,\n",
        "    KeypointsRandomAffineTransform,\n",
        "    KeypointsLongestMaxSize,\n",
        "    KeypointsPadIfNeeded,\n",
        "    KeypointsImageStandardize,\n",
        "    KeypointsImageNormalize,\n",
        "    KeypointsRemoveSmallObjects\n",
        ")\n",
        "\n",
        "# Indexes of keypoints on the flipped image. When doing left-right flip, left hand becomes right hand.\n",
        "#So this array contains order of keypoints on the flipped image. This is dataset specific and depends on\n",
        "#how keypoints are defined in dataset.\n",
        "#keypoints_random_horizontal_flip = KeypointsRandomHorizontalFlip(flip_index=config['flip_indexes'], prob=0.5)\n",
        "\n",
        "keypoints_hsv = KeypointsHSV(prob=0.5, hgain=20, sgain=20, vgain=20)\n",
        "\n",
        "keypoints_brightness_contrast = KeypointsBrightnessContrast(prob=0.5,\n",
        "                                                            brightness_range=[0.8, 1.2],\n",
        "                                                            contrast_range=[0.8, 1.2]\n",
        "                                                            )\n",
        "\n",
        "keypoints_mosaic = KeypointsMosaic(prob=0.8)\n",
        "\n",
        "keypoints_random_affine_transform = KeypointsRandomAffineTransform(max_rotation=0,\n",
        "                                                                   min_scale=0.5,\n",
        "                                                                   max_scale=1.5,\n",
        "                                                                   max_translate=0.1,\n",
        "                                                                   image_pad_value=127,\n",
        "                                                                   mask_pad_value=1,\n",
        "                                                                   prob=0.75,\n",
        "                                                                   interpolation_mode=[0, 1, 2, 3, 4]\n",
        "                                                                   )\n",
        "\n",
        "keypoints_longest_max_size = KeypointsLongestMaxSize(max_height=640, max_width=640)\n",
        "\n",
        "keypoints_pad_if_needed = KeypointsPadIfNeeded(min_height=640,\n",
        "                                               min_width=640,\n",
        "                                               image_pad_value=[127, 127, 127],\n",
        "                                               mask_pad_value=1,\n",
        "                                               padding_mode='bottom_right'\n",
        "                                               )\n",
        "\n",
        "keypoints_image_standardize = KeypointsImageStandardize(max_value=255)\n",
        "\n",
        "# keypoints_image_normalize = KeypointsImageNormalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                                     std=[0.229, 0.224, 0.225]\n",
        "#                                                     )\n",
        "\n",
        "keypoints_remove_small_objects = KeypointsRemoveSmallObjects(min_instance_area=1,\n",
        "                                                             min_visible_keypoints=1\n",
        "                                                             )"
      ],
      "metadata": {
        "id": "R14hnNdBSOiu",
        "outputId": "047a41f8-032e-4528-9fb7-aa7948960040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'super_gradients'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2075300975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Expand this cell to see how the transforms are instantiated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from super_gradients.training.transforms.keypoints import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mKeypointsHSV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mKeypointsBrightnessContrast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mKeypointsMosaic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = [\n",
        "    keypoints_hsv,\n",
        "    keypoints_brightness_contrast,\n",
        "    keypoints_mosaic,\n",
        "    keypoints_random_affine_transform,\n",
        "    keypoints_longest_max_size,\n",
        "    keypoints_pad_if_needed,\n",
        "    keypoints_image_standardize,\n",
        "    keypoints_remove_small_objects\n",
        "]\n",
        "\n",
        "val_transforms = [\n",
        "    keypoints_longest_max_size,\n",
        "    keypoints_pad_if_needed,\n",
        "    keypoints_image_standardize,\n",
        "]"
      ],
      "metadata": {
        "id": "S6hCFEGWSc3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGE FOR OUR CUSTOM DATA\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Dataset\"\n",
        "\n",
        "# Create instances of the dataset\n",
        "train_dataset = PoseEstimationDataset(\n",
        "    data_dir=data_path,\n",
        "    images_dir= data_path + '/train',\n",
        "    json_file= data_path + '/train/keypoint_train.json',\n",
        "    transforms=train_transforms,\n",
        "    edge_links = config['edge_links'],\n",
        "    edge_colors = config['edge_colors'],\n",
        "    keypoint_colors = config['keypoint_colors']\n",
        "    )\n",
        "\n",
        "val_dataset = PoseEstimationDataset(\n",
        "    data_dir='/content/drive/MyDrive/Dataset',\n",
        "    images_dir= data_path + '/val',\n",
        "    json_file= data_path + '/val/keypoint_val.json',\n",
        "    transforms=val_transforms,\n",
        "    edge_links = config['edge_links'],\n",
        "    edge_colors = config['edge_colors'],\n",
        "    keypoint_colors = config['keypoint_colors']\n",
        "    )\n",
        "\n",
        "test_dataset = PoseEstimationDataset(\n",
        "    data_dir='/content/drive/MyDrive/Dataset',\n",
        "    images_dir= data_path + '/val',\n",
        "    json_file= data_path + '/val/keypoint_val.json',\n",
        "    transforms=val_transforms,\n",
        "    edge_links = config['edge_links'],\n",
        "    edge_colors = config['edge_colors'],\n",
        "    keypoint_colors = config['keypoint_colors']\n",
        "    )"
      ],
      "metadata": {
        "id": "EIpRweKiSeXv",
        "outputId": "60a5c136-1420-4026-a7c8-daad23866422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PoseEstimationDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2957469931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create instances of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m train_dataset = PoseEstimationDataset(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimages_dir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PoseEstimationDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader_params = {\n",
        "    'shuffle': True,\n",
        "    'batch_size': 16,\n",
        "    'drop_last': True,\n",
        "    'pin_memory': False,\n",
        "    'collate_fn': YoloNASPoseCollateFN()\n",
        "    }\n",
        "\n",
        "val_dataloader_params = {\n",
        "    'shuffle': True,\n",
        "    'batch_size': 16,\n",
        "    'drop_last': True,\n",
        "    'pin_memory': False,\n",
        "    'collate_fn': YoloNASPoseCollateFN()\n",
        "    }\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, **train_dataloader_params)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, **val_dataloader_params)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, **val_dataloader_params)"
      ],
      "metadata": {
        "id": "U-DvAqPZSo3R",
        "outputId": "5d4fd82c-235b-44e0-8e3b-d34f657ef6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'YoloNASPoseCollateFN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-203969243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m'drop_last'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m'pin_memory'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m'collate_fn'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mYoloNASPoseCollateFN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'YoloNASPoseCollateFN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the model"
      ],
      "metadata": {
        "id": "Z4NWDZ0PSuRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_nas_pose = models.get(\"yolo_nas_pose_l\",\n",
        "                           num_classes=config['num_joints'],\n",
        "                           pretrained_weights=\"coco_pose\").cuda()"
      ],
      "metadata": {
        "id": "nDD2-jMySxAO",
        "outputId": "9e28a93a-f27c-4e65-daed-a6f5a4cd91aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-631207976.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m yolo_nas_pose = models.get(\"yolo_nas_pose_l\",\n\u001b[0m\u001b[1;32m      2\u001b[0m                            \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_joints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                            pretrained_weights=\"coco_pose\").cuda()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training parameters"
      ],
      "metadata": {
        "id": "AF3eA3k5S5CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Expand this cell to see the training params\n",
        "from super_gradients.training.models.pose_estimation_models.yolo_nas_pose import YoloNASPosePostPredictionCallback\n",
        "from super_gradients.training.utils.callbacks import ExtremeBatchPoseEstimationVisualizationCallback, Phase\n",
        "from super_gradients.training.utils.early_stopping import EarlyStop\n",
        "from super_gradients.training.metrics import PoseEstimationMetrics\n",
        "\n",
        "# Note: after next release unwrap all lines wrapped in oc.OmegaConf.create\n",
        "import omegaconf as oc\n",
        "\n",
        "post_prediction_callback = YoloNASPosePostPredictionCallback(\n",
        "  pose_confidence_threshold = 0.01,\n",
        "  nms_iou_threshold = 0.7,\n",
        "  pre_nms_max_predictions = 300,\n",
        "  post_nms_max_predictions = 30,\n",
        ")\n",
        "\n",
        "metrics = PoseEstimationMetrics(\n",
        "  num_joints = config['num_joints'],\n",
        "  oks_sigmas = config['oks_sigmas'],\n",
        "  max_objects_per_image = 30,\n",
        "  post_prediction_callback = post_prediction_callback,\n",
        ")\n",
        "\n",
        "visualization_callback = ExtremeBatchPoseEstimationVisualizationCallback(\n",
        "  keypoint_colors = config[\"keypoint_colors\"],\n",
        "  edge_colors = config['edge_colors'],\n",
        "  edge_links = config['edge_links'],\n",
        "  loss_to_monitor = \"YoloNASPoseLoss/loss\",\n",
        "  max = True,\n",
        "  freq = 1,\n",
        "  max_images = 1,\n",
        "  enable_on_train_loader = True,\n",
        "  enable_on_valid_loader = True,\n",
        "  post_prediction_callback = post_prediction_callback,\n",
        ")\n",
        "\n",
        "early_stop = EarlyStop(\n",
        "  phase = Phase.VALIDATION_EPOCH_END,\n",
        "  monitor = \"AP\",\n",
        "  mode = \"max\",\n",
        "  min_delta = 0.0001,\n",
        "  patience = 100,\n",
        "  verbose = True,\n",
        ")\n",
        "\n",
        "train_params = {\n",
        "    \"warmup_mode\": \"LinearBatchLRWarmup\",\n",
        "    \"warmup_initial_lr\": 1e-8,\n",
        "    \"lr_warmup_epochs\": 1,\n",
        "    \"initial_lr\": 5e-5,\n",
        "    \"lr_mode\": \"cosine\",\n",
        "    \"cosine_final_lr_ratio\": 5e-3,\n",
        "    \"max_epochs\": 5,\n",
        "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
        "    \"batch_accumulate\": 1,\n",
        "    \"average_best_models\": True,\n",
        "    \"save_ckpt_epoch_list\": [5, 10, 15, 20],\n",
        "    \"loss\": \"yolo_nas_pose_loss\",\n",
        "    \"criterion_params\": {\n",
        "        \"oks_sigmas\": config['oks_sigmas'],\n",
        "        \"classification_loss_weight\": 1.0,\n",
        "        \"classification_loss_type\": \"focal\",\n",
        "        \"regression_iou_loss_type\": \"ciou\",\n",
        "        \"iou_loss_weight\": 2.5,\n",
        "        \"dfl_loss_weight\": 0.01,\n",
        "        \"pose_cls_loss_weight\": 1.0,\n",
        "        \"pose_reg_loss_weight\": 34.0,\n",
        "        \"pose_classification_loss_type\": \"focal\",\n",
        "        \"rescale_pose_loss_with_assigned_score\": True,\n",
        "        \"assigner_multiply_by_pose_oks\": True,\n",
        "    },\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"optimizer_params\": {\n",
        "        \"weight_decay\": 0.000001\n",
        "    },\n",
        "    \"ema\": True,\n",
        "    \"ema_params\": {\n",
        "        \"decay\": 0.997,\n",
        "        \"decay_type\": \"threshold\"\n",
        "    },\n",
        "    \"mixed_precision\": True,\n",
        "    \"sync_bn\": False,\n",
        "    \"valid_metrics_list\": [metrics],\n",
        "    \"phase_callbacks\": [visualization_callback, early_stop],\n",
        "    \"pre_prediction_callback\": None,\n",
        "    \"metric_to_watch\": \"AP\",\n",
        "    \"greater_metric_to_watch_is_better\": True,\n",
        "    \"_convert_\": \"all\"\n",
        "}"
      ],
      "metadata": {
        "id": "WrzxEtOzS-9t",
        "outputId": "33791e91-d829-4a61-cbc0-bddf1bef91ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'super_gradients'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-544743097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Expand this cell to see the training params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose_estimation_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo_nas_pose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYoloNASPosePostPredictionCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtremeBatchPoseEstimationVisualizationCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPhase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPoseEstimationMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "asKFCOTaTDT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note, this is training for 10 epochs to demonstrate how to do it -> Change to fewer epochs\n",
        "trainer.train(model=yolo_nas_pose,\n",
        "              training_params=train_params,\n",
        "              train_loader=train_dataloader,\n",
        "              valid_loader=val_dataloader\n",
        "              )"
      ],
      "metadata": {
        "id": "mRJU8A5gTFOf",
        "outputId": "fef8572d-3815-44c2-ea8c-297b6cbab43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1199108139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note, this is training for 10 epochs to demonstrate how to do it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trainer.train(model=yolo_nas_pose,\n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0mtraining_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the best trained model"
      ],
      "metadata": {
        "id": "SSu42NXRTTqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = models.get('yolo_nas_pose_l',\n",
        "                        num_classes=config['num_joints'],\n",
        "                        checkpoint_path=\"/content/checkpoints/first_yn_pose_run/RUN_20240205_192025_039799/ckpt_best.pth\")"
      ],
      "metadata": {
        "id": "5crEDfmgTVk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the best trained model"
      ],
      "metadata": {
        "id": "rayiVUzfTbTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_prediction_callback = YoloNASPosePostPredictionCallback(\n",
        "  pose_confidence_threshold = 0.01,\n",
        "  nms_iou_threshold = 0.7,\n",
        "  pre_nms_max_predictions = 300,\n",
        "  post_nms_max_predictions = 30,\n",
        ")\n",
        "\n",
        "metrics = PoseEstimationMetrics(\n",
        "  num_joints = config['num_joints'],\n",
        "  oks_sigmas = config['oks_sigmas'],\n",
        "  max_objects_per_image = 30,\n",
        "  post_prediction_callback = post_prediction_callback,\n",
        ")\n",
        "\n",
        "trainer.test(model=best_model,\n",
        "            test_loader=test_dataloader,\n",
        "            test_metrics_list=metrics)"
      ],
      "metadata": {
        "id": "ZIVkrgLBTHTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting with the best model"
      ],
      "metadata": {
        "id": "818sG2-UTjb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change for our model\n",
        "\n",
        "img_url = \"/content/drive/MyDrive/Dataset/valid\"\n",
        "best_model.predict(img_url, conf=0.20).show()"
      ],
      "metadata": {
        "id": "sfhBMKjZTlqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}