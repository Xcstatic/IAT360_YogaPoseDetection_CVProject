{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3BCBLWEHPxeW"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anddennn/IAT360_YogaPoseDetection_CVProject/blob/main/ComputerVisionProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1> <b> YOLO-NAS-POSE Yoga Detection Model <b> </h1></center>"
      ],
      "metadata": {
        "id": "URt4Q-TNmNDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this youtube video as reference: https://www.youtube.com/watch?v=J83ZvWfxjoA\n",
        "\n",
        "Code snippets from video's linked google colab folder."
      ],
      "metadata": {
        "id": "f4zJQE__KpaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing required Libraries"
      ],
      "metadata": {
        "id": "rr-j-jCmaHId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install super-gradients\n",
        "!pip install -qq gdown torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzmALoxKXZh",
        "outputId": "956ca54e-030d-40ca-ae9d-c39a78e8a93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting super-gradients\n",
            "  Using cached super_gradients-3.7.1-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (4.67.1)\n",
            "Collecting boto3>=1.17.15 (from super-gradients)\n",
            "  Using cached boto3-1.40.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (4.25.1)\n",
            "Collecting Deprecated>=1.2.11 (from super-gradients)\n",
            "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (1.16.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (3.10.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (5.9.5)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.19.0)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (75.2.0)\n",
            "Collecting coverage~=5.3.1 (from super-gradients)\n",
            "  Using cached coverage-5.3.1-py3-none-any.whl\n",
            "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (0.23.0+cu126)\n",
            "Collecting sphinx~=4.0.2 (from super-gradients)\n",
            "  Using cached Sphinx-4.0.3-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting sphinx-rtd-theme (from super-gradients)\n",
            "  Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting torchmetrics==0.8 (from super-gradients)\n",
            "  Using cached torchmetrics-0.8.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting hydra-core>=1.2.0 (from super-gradients)\n",
            "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of super-gradients to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting super-gradients\n",
            "  Using cached super_gradients-3.7.0-py3-none-any.whl.metadata (41 kB)\n",
            "  Using cached super_gradients-3.6.1-py3-none-any.whl.metadata (41 kB)\n",
            "  Using cached super_gradients-3.6.0-py3-none-any.whl.metadata (40 kB)\n",
            "  Using cached super_gradients-3.5.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (4.12.0.88)\n",
            "  Using cached super_gradients-3.4.1-py3-none-any.whl.metadata (37 kB)\n",
            "  Using cached super_gradients-3.4.0-py3-none-any.whl.metadata (37 kB)\n",
            "  Using cached super_gradients-3.3.1-py3-none-any.whl.metadata (37 kB)\n",
            "INFO: pip is still looking at multiple versions of super-gradients to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached super_gradients-3.3.0-py3-none-any.whl.metadata (37 kB)\n",
            "  Using cached super_gradients-3.2.1-py3-none-any.whl.metadata (35 kB)\n",
            "  Using cached super_gradients-3.2.0-py3-none-any.whl.metadata (35 kB)\n",
            "  Using cached super_gradients-3.1.3-py3-none-any.whl.metadata (35 kB)\n",
            "  Using cached super_gradients-3.1.2-py3-none-any.whl.metadata (35 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached super_gradients-3.1.1-py3-none-any.whl.metadata (36 kB)\n",
            "  Using cached super_gradients-3.1.0-py3-none-any.whl.metadata (36 kB)\n",
            "  Using cached super_gradients-3.0.9-py3-none-any.whl.metadata (34 kB)\n",
            "  Using cached super_gradients-3.0.8-py3-none-any.whl.metadata (32 kB)\n",
            "  Using cached super_gradients-3.0.7-py3-none-any.whl.metadata (32 kB)\n",
            "  Using cached super_gradients-3.0.6-py3-none-any.whl.metadata (30 kB)\n",
            "  Using cached super_gradients-3.0.5-py3-none-any.whl.metadata (30 kB)\n",
            "  Using cached super_gradients-3.0.4-py3-none-any.whl.metadata (30 kB)\n",
            "  Using cached super_gradients-3.0.3-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: pillow>=9.2.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (11.3.0)\n",
            "Collecting onnxruntime (from super-gradients)\n",
            "  Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting onnx>=1.10.1 (from super-gradients)\n",
            "  Using cached onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting pip-tools>=6.4.0 (from super-gradients)\n",
            "  Using cached pip_tools-7.5.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting pyparsing==2.4.5 (from super-gradients)\n",
            "  Using cached pyparsing-2.4.5-py2.py3-none-any.whl.metadata (3.6 kB)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/einops/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting einops==0.3.2 (from super-gradients)\n",
            "  Using cached einops-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pycocotools==2.0.4 (from super-gradients)\n",
            "  Using cached pycocotools-2.0.4.tar.gz (106 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting protobuf~=3.19.0 (from super-gradients)\n",
            "  Using cached protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)\n",
            "Collecting treelib==1.6.1 (from super-gradients)\n",
            "  Using cached treelib-1.6.1-py3-none-any.whl\n",
            "Collecting termcolor==1.1.0 (from super-gradients)\n",
            "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.4 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (25.0)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (0.45.1)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from super-gradients) (2.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools==2.0.4->super-gradients) (2.0.2)\n",
            "Collecting pyDeprecate==0.3.* (from torchmetrics==0.8->super-gradients)\n",
            "  Using cached pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from treelib==1.6.1->super-gradients) (1.0.0)\n",
            "Collecting botocore<1.41.0,>=1.40.59 (from boto3>=1.17.15->super-gradients)\n",
            "  Using cached botocore-1.40.59-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.17.15->super-gradients)\n",
            "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3>=1.17.15->super-gradients)\n",
            "  Using cached s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting wrapt<2,>=1.10 (from Deprecated>=1.2.11->super-gradients)\n",
            "  Using cached wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.2.0->super-gradients) (4.9.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.2.0->super-gradients) (0.27.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (1.4.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.4->super-gradients) (2.9.0.post0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf->super-gradients) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting onnx>=1.10.1 (from super-gradients)\n",
            "  Using cached onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "  Using cached onnx-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Using cached onnx-1.17.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "  Using cached onnx-1.15.0.tar.gz (12.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is still looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached onnx-1.14.1.tar.gz (11.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.14.0.tar.gz (11.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.13.1.tar.gz (10.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.13.0.tar.gz (10.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Using cached onnx-1.12.0.tar.gz (10.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.10.1->super-gradients) (4.15.0)\n",
            "Requirement already satisfied: build>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (1.3.0)\n",
            "Requirement already satisfied: click>=8 in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (8.3.0)\n",
            "Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (24.1.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from pip-tools>=6.4.0->super-gradients) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (3.1.6)\n",
            "Collecting docutils<0.18,>=0.14 (from sphinx~=4.0.2->super-gradients)\n",
            "  Using cached docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (3.0.1)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.17.0)\n",
            "Collecting alabaster<0.8,>=0.7 (from sphinx~=4.0.2->super-gradients)\n",
            "  Using cached alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from sphinx~=4.0.2->super-gradients) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (3.9)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.4.1->super-gradients) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.9.0->super-gradients) (3.4.0)\n",
            "Collecting coloredlogs (from onnxruntime->super-gradients)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->super-gradients) (25.9.23)\n",
            "INFO: pip is looking at multiple versions of sphinx-rtd-theme to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sphinx-rtd-theme (from super-gradients)\n",
            "  Using cached sphinx_rtd_theme-3.0.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "  Using cached sphinx_rtd_theme-3.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "  Using cached sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "  Using cached sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->super-gradients)\n",
            "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.59->boto3>=1.17.15->super-gradients) (2.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.3->sphinx~=4.0.2->super-gradients) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->sphinx~=4.0.2->super-gradients) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.9.0->super-gradients) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->super-gradients)\n",
            "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Using cached super_gradients-3.0.3-py3-none-any.whl (732 kB)\n",
            "Using cached einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Using cached pyparsing-2.4.5-py2.py3-none-any.whl (67 kB)\n",
            "Using cached torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
            "Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Using cached boto3-1.40.59-py3-none-any.whl (139 kB)\n",
            "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Using cached pip_tools-7.5.1-py3-none-any.whl (65 kB)\n",
            "Using cached protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
            "Using cached Sphinx-4.0.3-py3-none-any.whl (2.9 MB)\n",
            "Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "Using cached sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
            "Using cached alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
            "Using cached botocore-1.40.59-py3-none-any.whl (14.1 MB)\n",
            "Using cached docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
            "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Using cached s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "Using cached wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Building wheels for collected packages: pycocotools, onnx\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycocotools \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pycocotools\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for onnx (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for onnx\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for onnx\n",
            "Failed to build pycocotools onnx\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pycocotools, onnx)\u001b[0m\u001b[31m\n",
            "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4373aa2",
        "outputId": "0d5abc54-dded-450b-fd02-d10f6bfc366b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycocotools==2.0.4 --global-option=\"build_ext\" --global-option=\"-I/usr/include/python3.10\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: --build-option and --global-option are deprecated. pip 24.2 will enforce this behaviour change. A possible replacement is to use --config-settings. Discussion can be found at https://github.com/pypa/pip/issues/11859\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pycocotools==2.0.4\n",
            "  Using cached pycocotools-2.0.4.tar.gz (106 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import Any, List, Tuple, Union\n",
        "\n",
        "# Third-party imports\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from IPython.display import YouTubeVideo\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Colab specific imports\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Constants\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "3_BT3x6HaFLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training import models\n",
        "from super_gradients.common.object_names import Models\n",
        "\n",
        "yolo_nas_pose = models.get(\"yolo_nas_pose_l\", pretrained_weights=\"coco_pose\").cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3lxnfMWALE9z",
        "outputId": "b205f146-357b-48fb-c2b8-1ad030ba935b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'super_gradients'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3402013858.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper_gradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_names\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0myolo_nas_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolo_nas_pose_l\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"coco_pose\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L3FGskainy8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "For preprocessing:\n",
        "- First manually sorted through the images to remove invalid data (i.e. clipart, children, non-yoga poses, etc.)\n",
        "- Added more diversity to the dataset\n",
        "- After annotation, used roboflow to normalize all data into 640x640 jpg images that are auto oriented for better performance\n",
        "- Augmented data with roboflow to mimic indoor conditions such as adding more low light data or low resolution data.\n"
      ],
      "metadata": {
        "id": "7aQjNM0hDaBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training import Trainer\n",
        "\n",
        "CHECKPOINT_DIR = 'checkpoints'\n",
        "trainer = Trainer(experiment_name='first_yn_pose_run', ckpt_root_dir=CHECKPOINT_DIR)"
      ],
      "metadata": {
        "id": "HpNoSkncPfQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading dataset"
      ],
      "metadata": {
        "id": "8baBWY6dPjiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7kHMqGVPPo1Q",
        "outputId": "49035db2-ac67-4a6f-ed29-aaa519835632",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "# fix path to dataset\n",
        "!cp -r \"/content/drive/MyDrive/Dataset\" \"/content/\""
      ],
      "metadata": {
        "id": "YiIr3J6rPpWk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"1OkaP72I2Cr9MhYbZuXi\")\n",
        "project = rf.workspace(\"yoga-pose-dataset\").project(\"yoga-poses-yiqyx\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")\n"
      ],
      "metadata": {
        "id": "Uyis1GKeKMgT",
        "outputId": "6d9a6a2e-946b-4cbb-c246-19b48b723b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2025.10.5)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp312-cp312-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "idna"
                ]
              },
              "id": "74ab953e7e484909873ce74ac9cbdcab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Yoga-Poses-1 to coco:: 100%|██████████| 22700/22700 [00:02<00:00, 8637.76it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Yoga-Poses-1 in coco:: 100%|██████████| 662/662 [00:00<00:00, 4683.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get yaml file for standard coco keypoints.\n",
        "!wget https://raw.githubusercontent.com/Deci-AI/super-gradients/master/src/super_gradients/recipes/dataset_params/coco_pose_estimation_common_dataset_params.yaml"
      ],
      "metadata": {
        "id": "ljdUO37IQDer",
        "outputId": "2b4184c5-6e3c-4636-88c1-83677ea0a005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-25 21:43:14--  https://raw.githubusercontent.com/Deci-AI/super-gradients/master/src/super_gradients/recipes/dataset_params/coco_pose_estimation_common_dataset_params.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1937 (1.9K) [text/plain]\n",
            "Saving to: ‘coco_pose_estimation_common_dataset_params.yaml’\n",
            "\n",
            "coco_pose_estimatio 100%[===================>]   1.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-25 21:43:15 (34.8 MB/s) - ‘coco_pose_estimation_common_dataset_params.yaml’ saved [1937/1937]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def open_file(file_path: str) -> Union[dict, list, None]:\n",
        "    \"\"\"\n",
        "    Opens and reads the content of a JSON or YAML file.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): The path to the file.\n",
        "\n",
        "    Returns:\n",
        "    Union[dict, list, None]: The content of the file parsed to a dictionary or a list,\n",
        "                             or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            if file_path.endswith('.json'):\n",
        "                return json.load(file)\n",
        "            elif file_path.endswith('.yaml') or file_path.endswith('.yml'):\n",
        "                return yaml.safe_load(file)\n",
        "            else:\n",
        "                raise ValueError(f'Unsupported file format: {file_path}')\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred: {e}')\n",
        "        return None\n",
        "\n",
        "# Fix paths to dataset ###########\n",
        "\n",
        "annotations = open_file('/content/drive/MyDrive/Dataset/train/_annotations.coco.json')\n",
        "config = open_file('/content/drive/MyDrive/Dataset/valid/_annotations.coco.json')"
      ],
      "metadata": {
        "id": "VENnHqTzQMU2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotation File Breakdown:\n",
        "\n",
        "1. images:\n",
        "It's a dictionary where each entry maps an ID to a filename (typically an image filename). Each image in the dataset has a unique identifier, and this is a lookup between the ID and the filename.\n",
        "\n",
        "2. annotations:\n",
        "This is a list containing 6,117 items. Each item is a dictionary with details related to the annotations for a particular image.\n",
        "Each annotation contains an image_id, a list of keypoints, and num_keypoints value.\n",
        "\n",
        "3. categories:\n",
        "A list of categories for the dataset.\n",
        "Each category has a:\n",
        "- supercategory: A broader classification (like 'animal').\n",
        "- id: A unique identifier for the category.\n",
        "- name: The name of the category (e.g., 'dog', 'cat', 'sheep').\n",
        "- keypoints: A list of names for specific keypoints relevant to that category (like 'left_eye', 'right_eye', 'nose', etc.).\n",
        "- skeleton: A list of pairs, which are connections between keypoints."
      ],
      "metadata": {
        "id": "Y1Xj5N68Qs3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotting a sample of images\n",
        "\n",
        "def plot_random_images(data, image_base_dir=\"/content/images\"):\n",
        "    \"\"\"\n",
        "    Plots 5 random images for each category from the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - data: The JSON dataset containing image, annotation, and category details.\n",
        "    - image_base_dir: The base directory where the images are located.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a dictionary to map image IDs to filenames\n",
        "    image_id_to_filename = {image['id']: image['filename'] for image in data['images']}\n",
        "\n",
        "    # Extracting image_ids for each category\n",
        "    category_image_ids = {}\n",
        "    for category in data['categories']:\n",
        "        category_id = category['id']\n",
        "        category_name = category['name']\n",
        "        category_image_ids[category_name] = [anno['image_id'] for anno in data['annotations'] if anno['category_id'] == category_id]\n",
        "\n",
        "    # Randomly select 5 image_ids for each category\n",
        "    random_selected_ids = {}\n",
        "    for category_name, ids in category_image_ids.items():\n",
        "        random_selected_ids[category_name] = random.sample(ids, min(5, len(ids)))\n",
        "\n",
        "    # Number of categories\n",
        "    num_categories = len(random_selected_ids)\n",
        "\n",
        "    # Create a figure to plot the images\n",
        "    fig, axes = plt.subplots(num_categories, 5, figsize=(20, num_categories * 3))\n",
        "    if num_categories == 1:  # If there is only one category, axes will be 1D\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (category_name, ids) in enumerate(random_selected_ids.items()):\n",
        "        for j, image_id in enumerate(ids):\n",
        "            # Get the filename using the image_id_to_filename dictionary\n",
        "            filename = image_id_to_filename.get(image_id, \"Image_Not_Found.jpg\")\n",
        "\n",
        "            # Load and plot the image\n",
        "            img_path = os.path.join(image_base_dir, filename)\n",
        "            try:\n",
        "                img = mpimg.imread(img_path)\n",
        "                axes[i][j].imshow(img)\n",
        "            except FileNotFoundError:\n",
        "                axes[i][j].imshow(np.zeros((100, 100, 3)))  # Show an empty image if file is not found\n",
        "            axes[i][j].axis('off')\n",
        "            if j == 0:\n",
        "                axes[i][j].set_title(category_name)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3ZQwxy3BQxNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change path for this\n",
        "plot_random_images(data=annotations, image_base_dir=\"/content/drive/MyDrive/Dataset/train\")"
      ],
      "metadata": {
        "id": "Oa9zimu1ROyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets & DataLoaders\n",
        "\n",
        "SuperGradients is fully compatible with PyTorch Datasets and Dataloaders, so you can use your dataloaders as is.\n",
        "\n",
        "### SuperGradients also provides you with the `AbstractPoseEstimationDataset` class.\n",
        "\n",
        "This is an abstract class defines a blueprint for datasets related to pose estimation tasks. It's expected that concrete implementations of this class will be created for specific datasets.\n",
        "\n",
        "- **Inheritance**: It inherits from PyTorch's `Dataset` and `HasPreprocessingParams`.\n",
        "\n",
        "- **Initialization**:\n",
        "  - Takes in parameters like `transforms`, `num_joints`, `edge_links`, `edge_colors`, and `keypoint_colors`.\n",
        "  - Initializes instance variables and constructs a transform pipeline (`KeypointsCompose`).\n",
        "\n",
        "- **Abstract Methods (`__len__` and `load_sample`)**:\n",
        "  - These methods are declared but don't have a concrete implementation in this class. Your derived class from this abstract class is expected to provide an implementation for these methods.\n",
        "  \n",
        "- **`load_random_sample` Method**:\n",
        "  - This method is used to fetch a random sample from the dataset. It uses the `__len__` method to get the total number of samples and then randomly selects an index to retrieve using `load_sample`.\n",
        "\n",
        "- **`__getitem__` Method**:\n",
        "  - This method retrieves a sample given its index. It then applies the defined transformations on the sample and returns it. This method is crucial for PyTorch's DataLoader to fetch samples during training.\n",
        "\n",
        "- **`get_dataset_preprocessing_params` Method**:\n",
        "  - This method defines and returns preprocessing parameters for the dataset. It seems to construct a pipeline of preprocessing steps and their parameters.\n"
      ],
      "metadata": {
        "id": "bkeI_DyoRsu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Expand this cell to see how the `AnimalPoseEstimationDataset` is implemented\n",
        "from super_gradients.common.decorators.factory_decorator import resolve_param\n",
        "from super_gradients.common.factories.target_generator_factory import TargetGeneratorsFactory\n",
        "from super_gradients.common.factories.transforms_factory import TransformsFactory\n",
        "from super_gradients.common.object_names import Datasets\n",
        "from super_gradients.common.registry import register_dataset\n",
        "from super_gradients.training.transforms.keypoint_transforms import AbstractKeypointTransform\n",
        "from super_gradients.training.samples import PoseEstimationSample\n",
        "\n",
        "from super_gradients.training.datasets.pose_estimation_datasets.abstract_pose_estimation_dataset import AbstractPoseEstimationDataset\n",
        "\n",
        "from super_gradients.training.datasets.pose_estimation_datasets import YoloNASPoseCollateFN\n",
        "\n",
        "class PoseEstimationDataset(AbstractPoseEstimationDataset):\n",
        "    \"\"\"\n",
        "    Dataset class for training pose estimation models on Animal Pose dataset.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    @resolve_param(\"transforms\", TransformsFactory())\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        images_dir: str,\n",
        "        json_file: str,\n",
        "        transforms: List[AbstractKeypointTransform],\n",
        "        edge_links: Union[List[Tuple[int, int]], np.ndarray],\n",
        "        edge_colors: Union[List[Tuple[int, int, int]], np.ndarray, None],\n",
        "        keypoint_colors: Union[List[Tuple[int, int, int]], np.ndarray, None],\n",
        "    ):\n",
        "        \"\"\"\n",
        "\n",
        "        :param data_dir: Root directory of the COCO dataset\n",
        "        :param images_dir: path suffix to the images directory inside the data_dir\n",
        "        :param json_file: path suffix to the json file inside the data_dir\n",
        "        :param include_empty_samples: Not used, but exists for compatibility with COCO dataset config.\n",
        "        :param target_generator: Target generator that will be used to generate the targets for the model.\n",
        "            See DEKRTargetsGenerator for an example.\n",
        "        :param transforms: Transforms to be applied to the image & keypoints\n",
        "        \"\"\"\n",
        "        split_json_file = os.path.join(data_dir, json_file)\n",
        "\n",
        "        with open(split_json_file, \"r\") as f:\n",
        "            json_annotations = json.load(f)\n",
        "\n",
        "\n",
        "        joints = json_annotations[\"categories\"][0][\"keypoints\"]\n",
        "        num_joints = len(joints)\n",
        "\n",
        "        super().__init__(\n",
        "            transforms=transforms,\n",
        "            num_joints=num_joints,\n",
        "            edge_links=edge_links,\n",
        "            edge_colors=edge_colors,\n",
        "            keypoint_colors=keypoint_colors,\n",
        "        )\n",
        "\n",
        "        self.num_joints = num_joints\n",
        "        print(self.num_joints)\n",
        "\n",
        "\n",
        "        images_and_ids = []\n",
        "\n",
        "        for image in json_annotations[\"images\"]:\n",
        "          images_and_ids.append((image[\"id\"], os.path.join(data_dir, images_dir, image[\"filename\"])))\n",
        "        self.image_ids, self.image_files = zip(*images_and_ids)\n",
        "\n",
        "        self.annotations = []\n",
        "\n",
        "        for image_id in self.image_ids:\n",
        "            keypoints_per_image = []\n",
        "            bboxes_per_image = []\n",
        "\n",
        "            image_annotations = [ann for ann in json_annotations[\"annotations\"] if str(ann[\"image_id\"]) == str(image_id)]\n",
        "            for ann in image_annotations:\n",
        "                keypoints = np.array(ann[\"keypoints\"]).reshape(self.num_joints, 3)\n",
        "                x1, y1, x2, y2 = ann[\"bbox\"]\n",
        "\n",
        "                bbox_xywh = np.array([x1, y1, x2 - x1, y2 - y1])\n",
        "                keypoints_per_image.append(keypoints)\n",
        "                bboxes_per_image.append(bbox_xywh)\n",
        "\n",
        "            keypoints_per_image = np.array(keypoints_per_image, dtype=np.float32).reshape(-1, self.num_joints, 3)\n",
        "            bboxes_per_image = np.array(bboxes_per_image, dtype=np.float32).reshape(-1, 4)\n",
        "            annotation = keypoints_per_image, bboxes_per_image\n",
        "            self.annotations.append(annotation)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def load_sample(self, index) -> PoseEstimationSample:\n",
        "        file_path = self.image_files[index]\n",
        "        gt_joints, gt_bboxes = self.annotations[index]  # boxes in xywh format\n",
        "\n",
        "        gt_areas = np.array([box[2] * box[3] for box in gt_bboxes], dtype=np.float32)\n",
        "        gt_iscrowd = np.array([0] * len(gt_joints), dtype=bool)\n",
        "\n",
        "        image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
        "        mask = np.ones(image.shape[:2], dtype=np.float32)\n",
        "\n",
        "        return PoseEstimationSample(\n",
        "            image=image, mask=mask, joints=gt_joints, areas=gt_areas, bboxes_xywh=gt_bboxes, is_crowd=gt_iscrowd, additional_samples=None\n",
        "        )"
      ],
      "metadata": {
        "id": "tNoeQk-zR5oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for annotated files\n",
        "\n",
        "# Change Paths\n",
        "train_annotations = open_file('/content/drive/MyDrive/Dataset/train/_annotations.coco.json')\n",
        "val_annotations = open_file('/content/drive/MyDrive/Dataset/valid/_annotations.coco.json')"
      ],
      "metadata": {
        "id": "uBgKq4CBR-NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KeyPoint Tranformations\n",
        "\n",
        "Instantiate the transformations"
      ],
      "metadata": {
        "id": "najn_WPGSHfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Expand this cell to see how the transforms are instantiated\n",
        "from super_gradients.training.transforms.keypoints import (\n",
        "    KeypointsHSV,\n",
        "    KeypointsBrightnessContrast,\n",
        "    KeypointsMosaic,\n",
        "    KeypointsRandomAffineTransform,\n",
        "    KeypointsLongestMaxSize,\n",
        "    KeypointsPadIfNeeded,\n",
        "    KeypointsImageStandardize,\n",
        "    KeypointsImageNormalize,\n",
        "    KeypointsRemoveSmallObjects\n",
        ")\n",
        "\n",
        "# Indexes of keypoints on the flipped image. When doing left-right flip, left hand becomes right hand.\n",
        "#So this array contains order of keypoints on the flipped image. This is dataset specific and depends on\n",
        "#how keypoints are defined in dataset.\n",
        "#keypoints_random_horizontal_flip = KeypointsRandomHorizontalFlip(flip_index=config['flip_indexes'], prob=0.5)\n",
        "\n",
        "keypoints_hsv = KeypointsHSV(prob=0.5, hgain=20, sgain=20, vgain=20)\n",
        "\n",
        "keypoints_brightness_contrast = KeypointsBrightnessContrast(prob=0.5,\n",
        "                                                            brightness_range=[0.8, 1.2],\n",
        "                                                            contrast_range=[0.8, 1.2]\n",
        "                                                            )\n",
        "\n",
        "keypoints_mosaic = KeypointsMosaic(prob=0.8)\n",
        "\n",
        "keypoints_random_affine_transform = KeypointsRandomAffineTransform(max_rotation=0,\n",
        "                                                                   min_scale=0.5,\n",
        "                                                                   max_scale=1.5,\n",
        "                                                                   max_translate=0.1,\n",
        "                                                                   image_pad_value=127,\n",
        "                                                                   mask_pad_value=1,\n",
        "                                                                   prob=0.75,\n",
        "                                                                   interpolation_mode=[0, 1, 2, 3, 4]\n",
        "                                                                   )\n",
        "\n",
        "keypoints_longest_max_size = KeypointsLongestMaxSize(max_height=640, max_width=640)\n",
        "\n",
        "keypoints_pad_if_needed = KeypointsPadIfNeeded(min_height=640,\n",
        "                                               min_width=640,\n",
        "                                               image_pad_value=[127, 127, 127],\n",
        "                                               mask_pad_value=1,\n",
        "                                               padding_mode='bottom_right'\n",
        "                                               )\n",
        "\n",
        "keypoints_image_standardize = KeypointsImageStandardize(max_value=255)\n",
        "\n",
        "# keypoints_image_normalize = KeypointsImageNormalize(mean=[0.485, 0.456, 0.406],\n",
        "#                                                     std=[0.229, 0.224, 0.225]\n",
        "#                                                     )\n",
        "\n",
        "keypoints_remove_small_objects = KeypointsRemoveSmallObjects(min_instance_area=1,\n",
        "                                                             min_visible_keypoints=1\n",
        "                                                             )"
      ],
      "metadata": {
        "id": "R14hnNdBSOiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = [\n",
        "    keypoints_hsv,\n",
        "    keypoints_brightness_contrast,\n",
        "    keypoints_mosaic,\n",
        "    keypoints_random_affine_transform,\n",
        "    keypoints_longest_max_size,\n",
        "    keypoints_pad_if_needed,\n",
        "    keypoints_image_standardize,\n",
        "    keypoints_remove_small_objects\n",
        "]\n",
        "\n",
        "val_transforms = [\n",
        "    keypoints_longest_max_size,\n",
        "    keypoints_pad_if_needed,\n",
        "    keypoints_image_standardize,\n",
        "]"
      ],
      "metadata": {
        "id": "S6hCFEGWSc3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGE FOR OUR CUSTOM DATA\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Dataset\"\n",
        "\n",
        "# Create instances of the dataset\n",
        "train_dataset = PoseEstimationDataset(\n",
        "    data_dir=data_path,\n",
        "    images_dir= data_path + '/train',\n",
        "    json_file= data_path + '/train/keypoint_train.json',\n",
        "    transforms=train_transforms,\n",
        "    edge_links = config['edge_links'],\n",
        "    edge_colors = config['edge_colors'],\n",
        "    keypoint_colors = config['keypoint_colors']\n",
        "    )\n",
        "\n",
        "val_dataset = PoseEstimationDataset(\n",
        "    data_dir='/content/drive/MyDrive/Dataset',\n",
        "    images_dir= data_path + '/val',\n",
        "    json_file= data_path + '/val/keypoint_val.json',\n",
        "    transforms=val_transforms,\n",
        "    edge_links = config['edge_links'],\n",
        "    edge_colors = config['edge_colors'],\n",
        "    keypoint_colors = config['keypoint_colors']\n",
        "    )\n",
        "\n",
        "test_dataset = PoseEstimationDataset(\n",
        "    data_dir='/content/drive/MyDrive/Dataset',\n",
        "    images_dir= data_path + '/val',\n",
        "    json_file= data_path + '/val/keypoint_val.json',\n",
        "    transforms=val_transforms,\n",
        "    edge_links = config['edge_links'],\n",
        "    edge_colors = config['edge_colors'],\n",
        "    keypoint_colors = config['keypoint_colors']\n",
        "    )"
      ],
      "metadata": {
        "id": "EIpRweKiSeXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader_params = {\n",
        "    'shuffle': True,\n",
        "    'batch_size': 16,\n",
        "    'drop_last': True,\n",
        "    'pin_memory': False,\n",
        "    'collate_fn': YoloNASPoseCollateFN()\n",
        "    }\n",
        "\n",
        "val_dataloader_params = {\n",
        "    'shuffle': True,\n",
        "    'batch_size': 16,\n",
        "    'drop_last': True,\n",
        "    'pin_memory': False,\n",
        "    'collate_fn': YoloNASPoseCollateFN()\n",
        "    }\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, **train_dataloader_params)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, **val_dataloader_params)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, **val_dataloader_params)"
      ],
      "metadata": {
        "id": "U-DvAqPZSo3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the model"
      ],
      "metadata": {
        "id": "Z4NWDZ0PSuRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_nas_pose = models.get(\"yolo_nas_pose_l\",\n",
        "                           num_classes=config['num_joints'],\n",
        "                           pretrained_weights=\"coco_pose\").cuda()"
      ],
      "metadata": {
        "id": "nDD2-jMySxAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training parameters"
      ],
      "metadata": {
        "id": "AF3eA3k5S5CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Expand this cell to see the training params\n",
        "from super_gradients.training.models.pose_estimation_models.yolo_nas_pose import YoloNASPosePostPredictionCallback\n",
        "from super_gradients.training.utils.callbacks import ExtremeBatchPoseEstimationVisualizationCallback, Phase\n",
        "from super_gradients.training.utils.early_stopping import EarlyStop\n",
        "from super_gradients.training.metrics import PoseEstimationMetrics\n",
        "\n",
        "# Note: after next release unwrap all lines wrapped in oc.OmegaConf.create\n",
        "import omegaconf as oc\n",
        "\n",
        "post_prediction_callback = YoloNASPosePostPredictionCallback(\n",
        "  pose_confidence_threshold = 0.01,\n",
        "  nms_iou_threshold = 0.7,\n",
        "  pre_nms_max_predictions = 300,\n",
        "  post_nms_max_predictions = 30,\n",
        ")\n",
        "\n",
        "metrics = PoseEstimationMetrics(\n",
        "  num_joints = config['num_joints'],\n",
        "  oks_sigmas = config['oks_sigmas'],\n",
        "  max_objects_per_image = 30,\n",
        "  post_prediction_callback = post_prediction_callback,\n",
        ")\n",
        "\n",
        "visualization_callback = ExtremeBatchPoseEstimationVisualizationCallback(\n",
        "  keypoint_colors = config[\"keypoint_colors\"],\n",
        "  edge_colors = config['edge_colors'],\n",
        "  edge_links = config['edge_links'],\n",
        "  loss_to_monitor = \"YoloNASPoseLoss/loss\",\n",
        "  max = True,\n",
        "  freq = 1,\n",
        "  max_images = 1,\n",
        "  enable_on_train_loader = True,\n",
        "  enable_on_valid_loader = True,\n",
        "  post_prediction_callback = post_prediction_callback,\n",
        ")\n",
        "\n",
        "early_stop = EarlyStop(\n",
        "  phase = Phase.VALIDATION_EPOCH_END,\n",
        "  monitor = \"AP\",\n",
        "  mode = \"max\",\n",
        "  min_delta = 0.0001,\n",
        "  patience = 100,\n",
        "  verbose = True,\n",
        ")\n",
        "\n",
        "train_params = {\n",
        "    \"warmup_mode\": \"LinearBatchLRWarmup\",\n",
        "    \"warmup_initial_lr\": 1e-8,\n",
        "    \"lr_warmup_epochs\": 1,\n",
        "    \"initial_lr\": 5e-5,\n",
        "    \"lr_mode\": \"cosine\",\n",
        "    \"cosine_final_lr_ratio\": 5e-3,\n",
        "    \"max_epochs\": 5,\n",
        "    \"zero_weight_decay_on_bias_and_bn\": True,\n",
        "    \"batch_accumulate\": 1,\n",
        "    \"average_best_models\": True,\n",
        "    \"save_ckpt_epoch_list\": [5, 10, 15, 20],\n",
        "    \"loss\": \"yolo_nas_pose_loss\",\n",
        "    \"criterion_params\": {\n",
        "        \"oks_sigmas\": config['oks_sigmas'],\n",
        "        \"classification_loss_weight\": 1.0,\n",
        "        \"classification_loss_type\": \"focal\",\n",
        "        \"regression_iou_loss_type\": \"ciou\",\n",
        "        \"iou_loss_weight\": 2.5,\n",
        "        \"dfl_loss_weight\": 0.01,\n",
        "        \"pose_cls_loss_weight\": 1.0,\n",
        "        \"pose_reg_loss_weight\": 34.0,\n",
        "        \"pose_classification_loss_type\": \"focal\",\n",
        "        \"rescale_pose_loss_with_assigned_score\": True,\n",
        "        \"assigner_multiply_by_pose_oks\": True,\n",
        "    },\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"optimizer_params\": {\n",
        "        \"weight_decay\": 0.000001\n",
        "    },\n",
        "    \"ema\": True,\n",
        "    \"ema_params\": {\n",
        "        \"decay\": 0.997,\n",
        "        \"decay_type\": \"threshold\"\n",
        "    },\n",
        "    \"mixed_precision\": True,\n",
        "    \"sync_bn\": False,\n",
        "    \"valid_metrics_list\": [metrics],\n",
        "    \"phase_callbacks\": [visualization_callback, early_stop],\n",
        "    \"pre_prediction_callback\": None,\n",
        "    \"metric_to_watch\": \"AP\",\n",
        "    \"greater_metric_to_watch_is_better\": True,\n",
        "    \"_convert_\": \"all\"\n",
        "}"
      ],
      "metadata": {
        "id": "WrzxEtOzS-9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "asKFCOTaTDT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note, this is training for 10 epochs to demonstrate how to do it -> Change to fewer epochs\n",
        "trainer.train(model=yolo_nas_pose,\n",
        "              training_params=train_params,\n",
        "              train_loader=train_dataloader,\n",
        "              valid_loader=val_dataloader\n",
        "              )"
      ],
      "metadata": {
        "id": "mRJU8A5gTFOf",
        "outputId": "fef8572d-3815-44c2-ea8c-297b6cbab43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1199108139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note, this is training for 10 epochs to demonstrate how to do it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trainer.train(model=yolo_nas_pose,\n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0mtraining_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the best trained model"
      ],
      "metadata": {
        "id": "SSu42NXRTTqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = models.get('yolo_nas_pose_l',\n",
        "                        num_classes=config['num_joints'],\n",
        "                        checkpoint_path=\"/content/checkpoints/first_yn_pose_run/RUN_20240205_192025_039799/ckpt_best.pth\")"
      ],
      "metadata": {
        "id": "5crEDfmgTVk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the best trained model"
      ],
      "metadata": {
        "id": "rayiVUzfTbTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_prediction_callback = YoloNASPosePostPredictionCallback(\n",
        "  pose_confidence_threshold = 0.01,\n",
        "  nms_iou_threshold = 0.7,\n",
        "  pre_nms_max_predictions = 300,\n",
        "  post_nms_max_predictions = 30,\n",
        ")\n",
        "\n",
        "metrics = PoseEstimationMetrics(\n",
        "  num_joints = config['num_joints'],\n",
        "  oks_sigmas = config['oks_sigmas'],\n",
        "  max_objects_per_image = 30,\n",
        "  post_prediction_callback = post_prediction_callback,\n",
        ")\n",
        "\n",
        "trainer.test(model=best_model,\n",
        "            test_loader=test_dataloader,\n",
        "            test_metrics_list=metrics)"
      ],
      "metadata": {
        "id": "ZIVkrgLBTHTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting with the best model"
      ],
      "metadata": {
        "id": "818sG2-UTjb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change for our model\n",
        "\n",
        "img_url = \"/content/drive/MyDrive/Dataset/valid\"\n",
        "best_model.predict(img_url, conf=0.20).show()"
      ],
      "metadata": {
        "id": "sfhBMKjZTlqk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}